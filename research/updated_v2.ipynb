{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1d4851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported Libraries\n",
    "import time, warnings, itertools, matplotlib.patches as mpatches, matplotlib.pyplot as plt, numpy as np, pandas as pd, seaborn as sns\n",
    "from typing import Optional, Sequence, Dict, Tuple, List\n",
    "from scipy.stats import norm\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "\n",
    "# Model Development Modules Import\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# SKLearn Modules Import\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection._split import _BaseKFold  # for type hinting\n",
    "from sklearn.model_selection import (StratifiedKFold, GridSearchCV, ShuffleSplit, RandomizedSearchCV, \n",
    "    train_test_split, cross_val_score, learning_curve, cross_val_predict\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, f1_score, precision_score, average_precision_score,recall_score, roc_auc_score, \n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Import data\n",
    "df=pd.read_csv(\"input/creditcard.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aa7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eedc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good No Null Values!\n",
    "df.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a888a4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d67d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classes are heavily skewed we need to solve this issue later.\n",
    "print(\"No Frauds\", round(df[\"Class\"].value_counts()[0] / len(df) * 100, 2), \"% of the dataset\")\n",
    "print(\"Frauds\", round(df[\"Class\"].value_counts()[1] / len(df) * 100, 2), \"% of the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c2708",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=[\"#0101DF\", \"#DF0101\"]\n",
    "sns.countplot(x=\"Class\", data=df, palette=colors)\n",
    "plt.title(\"Class Distributions \\n (0: No Fraud || 1: Fraud)\", fontsize=12)\n",
    "fig, ax=plt.subplots(1, 2, figsize=(18, 4))\n",
    "\n",
    "amount_val=df[\"Amount\"].values\n",
    "time_val=df[\"Time\"].values\n",
    "\n",
    "sns.histplot(amount_val, ax=ax[0], color=\"r\")\n",
    "ax[0].set_title(\"Distribution of Transaction Amount\", fontsize=12)\n",
    "ax[0].set_xlim([min(amount_val), max(amount_val)])\n",
    "\n",
    "sns.histplot(time_val, ax=ax[1], color=\"b\")\n",
    "ax[1].set_title(\"Distribution of Transaction Time\", fontsize=12)\n",
    "ax[1].set_xlim([min(time_val), max(time_val)])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf1bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n",
    "\n",
    "std_scaler=StandardScaler()\n",
    "# RobustScaler is less prone to outliers.\n",
    "rob_scaler=RobustScaler()\n",
    "\n",
    "df[\"scaled_amount\"]=rob_scaler.fit_transform(df[\"Amount\"].values.reshape(-1, 1))\n",
    "df[\"scaled_time\"]=rob_scaler.fit_transform(df[\"Time\"].values.reshape(-1, 1))\n",
    "df.drop([\"Time\", \"Amount\"], axis=1, inplace=True)\n",
    "\n",
    "scaled_amount=df[\"scaled_amount\"]\n",
    "scaled_time=df[\"scaled_time\"]\n",
    "df.drop([\"scaled_amount\", \"scaled_time\"], axis=1, inplace=True)\n",
    "df.insert(0, \"scaled_amount\", scaled_amount)\n",
    "df.insert(1, \"scaled_time\", scaled_time)\n",
    "# Amount and Time are Scaled!\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61805fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(\"Class\", axis=1)\n",
    "y=df[\"Class\"]\n",
    "\n",
    "sss=StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    original_Xtrain, original_Xtest=X.iloc[train_index], X.iloc[test_index]\n",
    "    original_ytrain, original_ytest=y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# Convert to array\n",
    "original_Xtrain=original_Xtrain.values\n",
    "original_Xtest=original_Xtest.values\n",
    "original_ytrain=original_ytrain.values\n",
    "original_ytest=original_ytest.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9a5b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n",
    "# original_Xtrain, original_Xtest, original_ytrain, original_ytest=train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# See if both the train and test label distribution are similarly distributed\n",
    "train_unique_label, train_counts_label=np.unique(original_ytrain, return_counts=True)\n",
    "test_unique_label, test_counts_label=np.unique(original_ytest, return_counts=True)\n",
    "print(\"-\" * 100)\n",
    "\n",
    "print(\"Label Distributions: \\n\")\n",
    "print(train_counts_label / len(original_ytrain))\n",
    "print(test_counts_label / len(original_ytest))\n",
    "\n",
    "# Since our classes are highly skewed we should make them equivalent in order to have \n",
    "# a normal distribution of the classes.\n",
    "\n",
    "# Lets shuffle the data before creating the subsamples\n",
    "df=df.sample(frac=1)\n",
    "# amount of fraud classes 492 rows.\n",
    "fraud_df=df.loc[df[\"Class\"]==1]\n",
    "non_fraud_df=df.loc[df[\"Class\"]==0][:492]\n",
    "normal_distributed_df=pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "new_df=normal_distributed_df.sample(frac=1, random_state=42)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e474a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribution of the Classes in the subsample dataset\")\n",
    "print(new_df[\"Class\"].value_counts() / len(new_df))\n",
    "\n",
    "sns.countplot(x=\"Class\", data=new_df, palette=colors)\n",
    "plt.title(\"Equally Distributed Classes\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e606eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure we use the subsample in our correlation\n",
    "f, (ax1, ax2)=plt.subplots(2, 1, figsize=(24, 20))\n",
    "\n",
    "# Entire DataFrame\n",
    "corr=df.corr()\n",
    "sns.heatmap(corr, cmap=\"coolwarm_r\", annot_kws={\"size\": 20}, ax=ax1)\n",
    "ax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=12)\n",
    "\n",
    "sub_sample_corr=new_df.corr()\n",
    "sns.heatmap(sub_sample_corr, cmap=\"coolwarm_r\", annot_kws={\"size\": 20}, ax=ax2)\n",
    "ax2.set_title(\"SubSample Correlation Matrix \\n (use for reference)\", fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b39f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes=plt.subplots(ncols=4, figsize=(20, 4))\n",
    "\n",
    "# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n",
    "sns.boxplot(x=\"Class\", y=\"V17\", data=new_df, palette=colors, ax=axes[0])\n",
    "axes[0].set_title(\"V17 vs Class Negative Correlation\")\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V14\", data=new_df, palette=colors, ax=axes[1])\n",
    "axes[1].set_title(\"V14 vs Class Negative Correlation\")\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V12\", data=new_df, palette=colors, ax=axes[2])\n",
    "axes[2].set_title(\"V12 vs Class Negative Correlation\")\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V10\", data=new_df, palette=colors, ax=axes[3])\n",
    "axes[3].set_title(\"V10 vs Class Negative Correlation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5093ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes=plt.subplots(ncols=4, figsize=(20, 4))\n",
    "\n",
    "# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\n",
    "sns.boxplot(x=\"Class\", y=\"V11\", data=new_df, palette=colors, ax=axes[0])\n",
    "axes[0].set_title(\"V11 vs Class Positive Correlation\")\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V4\", data=new_df, palette=colors, ax=axes[1])\n",
    "axes[1].set_title(\"V4 vs Class Positive Correlation\")\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V2\", data=new_df, palette=colors, ax=axes[2])\n",
    "axes[2].set_title(\"V2 vs Class Positive Correlation\")\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V19\", data=new_df, palette=colors, ax=axes[3])\n",
    "axes[3].set_title(\"V19 vs Class Positive Correlation\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b6089",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3)=plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "# V14\n",
    "v14_fraud_dist=new_df[\"V14\"].loc[new_df[\"Class\"]==1].values\n",
    "sns.histplot(v14_fraud_dist, ax=ax1, kde=False, stat=\"density\", color=\"#FB8861\", bins=30)\n",
    "\n",
    "mu, std=norm.fit(v14_fraud_dist)\n",
    "xmin, xmax=ax1.get_xlim()\n",
    "x=np.linspace(xmin, xmax, 100)\n",
    "p=norm.pdf(x, mu, std)\n",
    "ax1.plot(x, p, 'k', linewidth=2)\n",
    "ax1.set_title(\"V14 Distribution\\n(Fraud Transactions)\", fontsize=12)\n",
    "\n",
    "# V12\n",
    "v12_fraud_dist=new_df[\"V12\"].loc[new_df[\"Class\"]==1].values\n",
    "sns.histplot(v12_fraud_dist, ax=ax2, kde=False, stat=\"density\", color=\"#56F9BB\", bins=30)\n",
    "\n",
    "mu, std=norm.fit(v12_fraud_dist)\n",
    "xmin, xmax=ax2.get_xlim()\n",
    "x=np.linspace(xmin, xmax, 100)\n",
    "p=norm.pdf(x, mu, std)\n",
    "ax2.plot(x, p, 'k', linewidth=2)\n",
    "ax2.set_title(\"V12 Distribution\\n(Fraud Transactions)\", fontsize=12)\n",
    "\n",
    "# V10\n",
    "v10_fraud_dist=new_df[\"V10\"].loc[new_df[\"Class\"]==1].values\n",
    "sns.histplot(v10_fraud_dist, ax=ax3, kde=False, stat=\"density\", color=\"#C5B3F9\", bins=30)\n",
    "\n",
    "mu, std=norm.fit(v10_fraud_dist)\n",
    "xmin, xmax=ax3.get_xlim()\n",
    "x=np.linspace(xmin, xmax, 100)\n",
    "p=norm.pdf(x, mu, std)\n",
    "ax3.plot(x, p, 'k', linewidth=2)\n",
    "ax3.set_title(\"V10 Distribution\\n(Fraud Transactions)\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520b003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\n",
    "v14_fraud=new_df[\"V14\"].loc[new_df[\"Class\"]==1].values\n",
    "q25, q75=np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\n",
    "print(f\"Quartile 25: {q25} | Quartile 75: {q75}\")\n",
    "v14_iqr=q75 - q25\n",
    "print(f\"iqr: {v14_iqr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87986b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "v14_cut_off=v14_iqr * 1.5\n",
    "v14_lower, v14_upper=q25 - v14_cut_off, q75 + v14_cut_off\n",
    "print(f\"Cut Off: {v14_cut_off}\")\n",
    "print(f\"V14 Lower: {v14_lower}\")\n",
    "print(f\"V14 Upper: {v14_upper}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fb4d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers=[x for x in v14_fraud if x < v14_lower or x > v14_upper]\n",
    "print(f\"Feature V14 Outliers for Fraud Cases: {len(outliers)}\")\n",
    "print(f\"V10 outliers:{outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01923771",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=new_df.drop(new_df[(new_df[\"V14\"] > v14_upper) | (new_df[\"V14\"] < v14_lower)].index)\n",
    "print(\"----\" * 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db90a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----> V12 removing outliers from fraud transactions\n",
    "v12_fraud=new_df[\"V12\"].loc[new_df[\"Class\"]==1].values\n",
    "q25, q75=np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\n",
    "\n",
    "v12_iqr=q75 - q25\n",
    "v12_cut_off=v12_iqr * 1.5\n",
    "v12_lower, v12_upper=q25 - v12_cut_off, q75 + v12_cut_off\n",
    "print(f\"V12 Lower: {v12_lower}\")\n",
    "print(f\"V12 Upper: {v12_upper}\")\n",
    "\n",
    "outliers=[x for x in v12_fraud if x < v12_lower or x > v12_upper]\n",
    "print(f\"V12 outliers: {outliers}\")\n",
    "print(f\"Feature V12 Outliers for Fraud Cases: {len(outliers)}\")\n",
    "\n",
    "new_df=new_df.drop(new_df[(new_df[\"V12\"] > v12_upper) | (new_df[\"V12\"] < v12_lower)].index)\n",
    "print(f\"Number of Instances after outliers removal: {len(new_df)}\")\n",
    "print(\"----\" * 44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf96c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing outliers V10 Feature\n",
    "v10_fraud=new_df[\"V10\"].loc[new_df[\"Class\"]==1].values\n",
    "q25, q75=np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\n",
    "\n",
    "v10_iqr=q75 - q25\n",
    "v10_cut_off=v10_iqr * 1.5\n",
    "v10_lower, v10_upper=q25 - v10_cut_off, q75 + v10_cut_off\n",
    "print(f\"V10 Lower: {v10_lower}\")\n",
    "print(f\"V10 Upper: {v10_upper}\")\n",
    "\n",
    "outliers=[x for x in v10_fraud if x < v10_lower or x > v10_upper]\n",
    "print(f\"V10 outliers: {outliers}\")\n",
    "print(f\"Feature V10 Outliers for Fraud Cases: {len(outliers)}\")\n",
    "\n",
    "new_df=new_df.drop(new_df[(new_df[\"V10\"] > v10_upper) | (new_df[\"V10\"] < v10_lower)].index)\n",
    "print(f\"Number of Instances after outliers removal: {len(new_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9c5fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3)=plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "colors=[\"#B3F9C5\", \"#f9c5b3\"]\n",
    "# Boxplots with outliers removed\n",
    "# Feature V14\n",
    "sns.boxplot(x=\"Class\", y=\"V14\", data=new_df, ax=ax1, palette=colors)\n",
    "ax1.set_title(\"V14 Feature Reduction of outliers\", fontsize=12)\n",
    "ax1.annotate(\"Fewer extreme outliers\", xy=(0.98, -17.5), xytext=(0, -12), arrowprops=dict(facecolor=\"black\"), fontsize=12)\n",
    "\n",
    "# Feature 12\n",
    "sns.boxplot(x=\"Class\", y=\"V12\", data=new_df, ax=ax2, palette=colors)\n",
    "ax2.set_title(\"V12 Feature Reduction of outliers\", fontsize=12)\n",
    "ax2.annotate(\"Fewer extreme outliers\", xy=(0.98, -17.3), xytext=(0, -12), arrowprops=dict(facecolor=\"black\"), fontsize=12)\n",
    "\n",
    "# Feature V10\n",
    "sns.boxplot(x=\"Class\", y=\"V10\", data=new_df, ax=ax3, palette=colors)\n",
    "ax3.set_title(\"V10 Feature Reduction of outliers\", fontsize=12)\n",
    "ax3.annotate(\"Fewer extreme outliers\", xy=(0.95, -16.5), xytext=(0, -12), arrowprops=dict(facecolor=\"black\"), fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New_df is from the random undersample data (fewer instances)\n",
    "X=new_df.drop(\"Class\", axis=1)\n",
    "y=new_df[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82e2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T-SNE Implementation\n",
    "t0=time.time()\n",
    "X_reduced_tsne=TSNE(n_components=2, random_state=42).fit_transform(X.values)\n",
    "t1=time.time()\n",
    "print(f\"T-SNE took {t1 - t0:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a74c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Implementation\n",
    "t0=time.time()\n",
    "X_reduced_pca=PCA(n_components=2, random_state=42).fit_transform(X.values)\n",
    "t1=time.time()\n",
    "print(f\"PCA took {t1 - t0:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d8032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TruncatedSVD\n",
    "t0=time.time()\n",
    "X_reduced_svd=TruncatedSVD(n_components=2, algorithm=\"randomized\", random_state=42).fit_transform(X.values)\n",
    "t1=time.time()\n",
    "print(f\"Truncated SVD took {t1 - t0:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3)=plt.subplots(1, 3, figsize=(24, 6))\n",
    "# labels=['No Fraud', 'Fraud']\n",
    "f.suptitle(\"Clusters using Dimensionality Reduction\", fontsize=14)\n",
    "\n",
    "blue_patch=mpatches.Patch(color=\"#0A0AFF\", label=\"No Fraud\")\n",
    "red_patch=mpatches.Patch(color=\"#AF0000\", label=\"Fraud\")\n",
    "\n",
    "# t-SNE scatter plot\n",
    "ax1.scatter(X_reduced_tsne[:, 0], X_reduced_tsne[:, 1], c=(y==0), cmap=\"coolwarm\", label=\"No Fraud\", linewidths=2)\n",
    "ax1.scatter(X_reduced_tsne[:, 0], X_reduced_tsne[:, 1], c=(y==1), cmap=\"coolwarm\", label=\"Fraud\", linewidths=2)\n",
    "ax1.set_title(\"t-SNE\", fontsize=14)\n",
    "ax1.grid(True)\n",
    "ax1.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "# PCA scatter plot\n",
    "ax2.scatter(X_reduced_pca[:, 0], X_reduced_pca[:, 1], c=(y==0), cmap=\"coolwarm\", label=\"No Fraud\", linewidths=2)\n",
    "ax2.scatter(X_reduced_pca[:, 0], X_reduced_pca[:, 1], c=(y==1), cmap=\"coolwarm\", label=\"Fraud\", linewidths=2)\n",
    "ax2.set_title(\"PCA\", fontsize=14)\n",
    "ax2.grid(True)\n",
    "ax2.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "# TruncatedSVD scatter plot\n",
    "ax3.scatter(X_reduced_svd[:, 0], X_reduced_svd[:, 1], c=(y==0), cmap=\"coolwarm\", label=\"No Fraud\", linewidths=2)\n",
    "ax3.scatter(X_reduced_svd[:, 0], X_reduced_svd[:, 1], c=(y==1), cmap=\"coolwarm\", label=\"Fraud\", linewidths=2)\n",
    "ax3.set_title(\"Truncated SVD\", fontsize=14)\n",
    "ax3.grid(True)\n",
    "ax3.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94307df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling before cross validating (prone to overfit)\n",
    "X=new_df.drop(\"Class\", axis=1)\n",
    "y=new_df[\"Class\"]\n",
    "\n",
    "# Our data is already scaled we should split our training and test sets\n",
    "# This is explicitly used for undersampling.\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Turn the values into an array for feeding the classification algorithms.\n",
    "X_train=X_train.values\n",
    "X_test=X_test.values\n",
    "y_train=y_train.values\n",
    "y_test=y_test.values\n",
    "\n",
    "# Let's implement simple classifiers\n",
    "classifiers={\n",
    "    \"LogisiticRegression\": LogisticRegression(), \n",
    "    \"KNearest\": KNeighborsClassifier(), \n",
    "    \"Support Vector Classifier\": SVC(), \n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "# Wow our scores are getting even high scores even when applying cross validation.\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score=cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    print(f\"Classifiers: {classifier.__class__.__name__} has a training score of {round(training_score.mean(), 2) * 100} % accuracy score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV to find the best parameters.\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg_params={\"penalty\": [\"l1\", \"l2\"], \"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "grid_log_reg=GridSearchCV(LogisticRegression(), log_reg_params)\n",
    "grid_log_reg.fit(X_train, y_train)\n",
    "# LogisticRegression best estimator\n",
    "log_reg=grid_log_reg.best_estimator_\n",
    "\n",
    "# KNN\n",
    "knears_params={\"n_neighbors\": list(range(2, 5, 1)), \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]}\n",
    "grid_knears=GridSearchCV(KNeighborsClassifier(), knears_params)\n",
    "grid_knears.fit(X_train, y_train)\n",
    "# KNears best estimator\n",
    "knears_neighbors=grid_knears.best_estimator_\n",
    "\n",
    "# Support Vector Classifier\n",
    "svc_params={\"C\": [0.5, 0.7, 0.9, 1], \"kernel\": [\"rbf\", \"poly\", \"sigmoid\", \"linear\"]}\n",
    "grid_svc=GridSearchCV(SVC(), svc_params)\n",
    "grid_svc.fit(X_train, y_train)\n",
    "# SVC best estimator\n",
    "svc=grid_svc.best_estimator_\n",
    "\n",
    "# DecisionTree Classifier\n",
    "tree_params={\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2, 4, 1)), \"min_samples_leaf\": list(range(5, 7, 1))}\n",
    "grid_tree=GridSearchCV(DecisionTreeClassifier(), tree_params)\n",
    "grid_tree.fit(X_train, y_train)\n",
    "# DecisionTree best estimator\n",
    "tree_clf=grid_tree.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7b0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting Case\n",
    "log_reg_score=cross_val_score(log_reg, X_train, y_train, cv=5)\n",
    "print(f\"Logistic Regression Cross Validation Score: {round(log_reg_score.mean() * 100, 2).astype(str)}%\")\n",
    "\n",
    "knears_score=cross_val_score(knears_neighbors, X_train, y_train, cv=5)\n",
    "print(f\"Knears Neighbors Cross Validation Score {round(knears_score.mean() * 100, 2).astype(str)} %\")\n",
    "\n",
    "svc_score=cross_val_score(svc, X_train, y_train, cv=5)\n",
    "print(f\"Support Vector Classifier Cross Validation Score{round(svc_score.mean() * 100, 2).astype(str)} %\")\n",
    "\n",
    "tree_score=cross_val_score(tree_clf, X_train, y_train, cv=5)\n",
    "print(f\"DecisionTree Classifier Cross Validation Score {round(tree_score.mean() * 100, 2).astype(str)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac3c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will undersample during cross validating\n",
    "undersample_X=df.drop(\"Class\", axis=1)\n",
    "undersample_y=df[\"Class\"]\n",
    "\n",
    "for train_index, test_index in sss.split(undersample_X, undersample_y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    undersample_Xtrain, undersample_Xtest=(undersample_X.iloc[train_index], undersample_X.iloc[test_index])\n",
    "    undersample_ytrain, undersample_ytest=(undersample_y.iloc[train_index], undersample_y.iloc[test_index])\n",
    "\n",
    "undersample_Xtrain=undersample_Xtrain.values\n",
    "undersample_Xtest=undersample_Xtest.values\n",
    "undersample_ytrain=undersample_ytrain.values\n",
    "undersample_ytest=undersample_ytest.values\n",
    "\n",
    "\n",
    "# Implementing NearMiss Technique\n",
    "# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\n",
    "X_nearmiss, y_nearmiss=NearMiss().fit_resample(undersample_X.values, undersample_y.values)\n",
    "print(f\"NearMiss Label Distribution: {Counter(y_nearmiss)}\")\n",
    "\n",
    "# Cross Validating the right way\n",
    "undersample_accuracy=[]\n",
    "undersample_precision=[]\n",
    "undersample_recall=[]\n",
    "undersample_f1=[]\n",
    "undersample_auc=[]\n",
    "\n",
    "for train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n",
    "    undersample_pipeline=imbalanced_make_pipeline(NearMiss(sampling_strategy=\"majority\"), log_reg)  # SMOTE happens during Cross Validation not before..\n",
    "    undersample_model=undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n",
    "    undersample_prediction=undersample_model.predict(undersample_Xtrain[test])\n",
    "\n",
    "    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n",
    "    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n",
    "    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n",
    "    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n",
    "    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Plot LogisticRegression Learning Curve\n",
    "# def plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None, n_jobs=1, train_sizes=np.linspace(0.1, 1.0, 5)):\n",
    "#     f, ((ax1, ax2), (ax3, ax4))=plt.subplots(2, 2, figsize=(20, 14), sharey=True)\n",
    "    \n",
    "#     if ylim is not None:\n",
    "#         plt.ylim(*ylim)\n",
    "    \n",
    "#     # First Estimator\n",
    "#     train_sizes, train_scores, test_scores=learning_curve(estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "#     train_scores_mean=np.mean(train_scores, axis=1)\n",
    "#     train_scores_std=np.std(train_scores, axis=1)\n",
    "#     test_scores_mean=np.mean(test_scores, axis=1)\n",
    "#     test_scores_std=np.std(test_scores, axis=1)\n",
    "#     ax1.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"#ff9124\")\n",
    "#     ax1.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
    "#     ax1.plot(train_sizes, train_scores_mean, \"o-\", color=\"#ff9124\", label=\"Training score\")\n",
    "#     ax1.plot(train_sizes, test_scores_mean, \"o-\", color=\"#2492ff\", label=\"Cross-validation score\")\n",
    "#     ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=12)\n",
    "#     ax1.set_xlabel(\"Training size (m)\")\n",
    "#     ax1.set_ylabel(\"Score\")\n",
    "#     ax1.grid(True)\n",
    "#     ax1.legend(loc=\"best\")\n",
    "\n",
    "#     # Second Estimator\n",
    "#     train_sizes, train_scores, test_scores=learning_curve(estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "#     train_scores_mean=np.mean(train_scores, axis=1)\n",
    "#     train_scores_std=np.std(train_scores, axis=1)\n",
    "#     test_scores_mean=np.mean(test_scores, axis=1)\n",
    "#     test_scores_std=np.std(test_scores, axis=1)\n",
    "#     ax2.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"#ff9124\")\n",
    "#     ax2.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
    "#     ax2.plot(train_sizes, train_scores_mean, \"o-\", color=\"#ff9124\", label=\"Training score\")\n",
    "#     ax2.plot(train_sizes, test_scores_mean, \"o-\", color=\"#2492ff\", label=\"Cross-validation score\")\n",
    "#     ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=12)\n",
    "#     ax2.set_xlabel(\"Training size (m)\")\n",
    "#     ax2.set_ylabel(\"Score\")\n",
    "#     ax2.grid(True)\n",
    "#     ax2.legend(loc=\"best\")\n",
    "\n",
    "#     # Third Estimator\n",
    "#     train_sizes, train_scores, test_scores=learning_curve(estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "#     train_scores_mean=np.mean(train_scores, axis=1)\n",
    "#     train_scores_std=np.std(train_scores, axis=1)\n",
    "#     test_scores_mean=np.mean(test_scores, axis=1)\n",
    "#     test_scores_std=np.std(test_scores, axis=1)\n",
    "#     ax3.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"#ff9124\")\n",
    "#     ax3.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
    "#     ax3.plot(train_sizes, train_scores_mean, \"o-\", color=\"#ff9124\", label=\"Training score\")\n",
    "#     ax3.plot(train_sizes, test_scores_mean, \"o-\", color=\"#2492ff\", label=\"Cross-validation score\")\n",
    "#     ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=12)\n",
    "#     ax3.set_xlabel(\"Training size (m)\")\n",
    "#     ax3.set_ylabel(\"Score\")\n",
    "#     ax3.grid(True)\n",
    "#     ax3.legend(loc=\"best\")\n",
    "\n",
    "#     # Fourth Estimator\n",
    "#     train_sizes, train_scores, test_scores=learning_curve(estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "#     train_scores_mean=np.mean(train_scores, axis=1)\n",
    "#     train_scores_std=np.std(train_scores, axis=1)\n",
    "#     test_scores_mean=np.mean(test_scores, axis=1)\n",
    "#     test_scores_std=np.std(test_scores, axis=1)\n",
    "#     ax4.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.1, color=\"#ff9124\")\n",
    "#     ax4.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n",
    "#     ax4.plot(train_sizes, train_scores_mean, \"o-\", color=\"#ff9124\", label=\"Training score\")\n",
    "#     ax4.plot( train_sizes, test_scores_mean, \"o-\", color=\"#2492ff\", label=\"Cross-validation score\")\n",
    "#     ax4.set_title(\"Decision Tree Classifier \\n Learning Curve\", fontsize=12)\n",
    "#     ax4.set_xlabel(\"Training size (m)\")\n",
    "#     ax4.set_ylabel(\"Score\")\n",
    "#     ax4.grid(True)\n",
    "#     ax4.legend(loc=\"best\")\n",
    "#     return plt\n",
    "\n",
    "# cv=ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\n",
    "# plot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def _plot_estimator_curve(ax: plt.Axes, estimator, X, y, cv: _BaseKFold, n_jobs: int, train_sizes: np.ndarray, title: str, ylim: Optional[Sequence[float]]=None) -> None:\n",
    "    \"\"\"\n",
    "    Helper to compute and plot a single estimator's learning curve on the given axis.\n",
    "\n",
    "    Args:\n",
    "        ax: Matplotlib axis to draw on.\n",
    "        estimator: A scikit-learn estimator with fit/predict methods.\n",
    "        X: Feature matrix.\n",
    "        y: Target vector.\n",
    "        cv: Cross-validation splitter instance.\n",
    "        n_jobs: Number of jobs for parallel processing.\n",
    "        train_sizes: Relative or absolute sizes of training sets.\n",
    "        title: Title for this subplot.\n",
    "        ylim: Optional tuple (ymin, ymax) to set y-axis limits.\n",
    "    \"\"\"\n",
    "    # Compute learning curve metrics\n",
    "    train_sizes_abs, train_scores, test_scores=learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "\n",
    "    # Calculate mean and std for training and validation scores\n",
    "    train_mean=np.mean(train_scores, axis=1)\n",
    "    train_std=np.std(train_scores, axis=1)\n",
    "    test_mean=np.mean(test_scores, axis=1)\n",
    "    test_std=np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot shading for std deviation\n",
    "    ax.fill_between(train_sizes_abs, train_mean - train_std, train_mean + train_std, alpha=0.1, color=\"#ff9124\")\n",
    "    ax.fill_between(train_sizes_abs, test_mean - test_std, test_mean + test_std, alpha=0.1, color=\"#2492ff\")\n",
    "    # Plot mean lines\n",
    "    ax.plot(train_sizes_abs, train_mean, marker=\"o\", linestyle=\"-\", color=\"#ff9124\", label=\"Training score\")\n",
    "    ax.plot(train_sizes_abs, test_mean, marker=\"o\", linestyle=\"-\", color=\"#2492ff\", label=\"Cross-validation score\")\n",
    "\n",
    "    # Set axis labels, title, grid, and legend\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    ax.set_xlabel(\"Training size (m)\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    if ylim is not None:\n",
    "        ax.set_ylim(*ylim)\n",
    "    ax.grid(True)\n",
    "    ax.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def plot_learning_curves(estimators: Sequence, titles: Sequence[str], X, y, ylim: Optional[Sequence[float]]=None, cv: Optional[_BaseKFold]=None, n_jobs: int=1, train_sizes: Optional[np.ndarray]=None) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot learning curves for multiple estimators in a 2x2 grid.\n",
    "\n",
    "    Args:\n",
    "        estimators: Sequence of four scikit-learn estimators.\n",
    "        titles: Sequence of four titles corresponding to each estimator.\n",
    "        X: Feature matrix.\n",
    "        y: Target vector.\n",
    "        ylim: Optional (ymin, ymax) for all plots.\n",
    "        cv: Cross-validation splitter. If None, defaults to None in sklearn.\n",
    "        n_jobs: Number of parallel jobs.\n",
    "        train_sizes: Array of training set sizes to evaluate; defaults to np.linspace(0.1, 1.0, 5).\n",
    "\n",
    "    Returns:\n",
    "        The matplotlib Figure object containing the plots.\n",
    "    \"\"\"\n",
    "    # Default train sizes from 10% to 100% of data\n",
    "    if train_sizes is None:\n",
    "        train_sizes=np.linspace(0.1, 1.0, 5)\n",
    "\n",
    "    # Create a 2x2 subplot grid\n",
    "    fig, axes=plt.subplots(2, 2, figsize=(20, 14), sharey=True)\n",
    "    axes_flat=axes.ravel()\n",
    "\n",
    "    # Plot each estimator\n",
    "    for ax, estimator, title in zip(axes_flat, estimators, titles):\n",
    "        _plot_estimator_curve(ax=ax, estimator=estimator, X=X, y=y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, title=title, ylim=ylim)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Example usage:\n",
    "cv=ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\n",
    "estimators=[log_reg, knears_neighbors, svc, tree_clf]\n",
    "titles=[\"Logistic Regression Learning Curve\", \"K-Neighbors Learning Curve\", \"Support Vector Classifier Learning Curve\", \"Decision Tree Classifier Learning Curve\"]\n",
    "\n",
    "fig=plot_learning_curves(estimators, titles, X_train, y_train, ylim=(0.87, 1.01), cv=cv, n_jobs=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7d6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with all the scores and the classifiers names.\n",
    "# log_reg_pred=cross_val_predict(log_reg, X_train, y_train, cv=5, method=\"decision_function\")\n",
    "# knears_pred=cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n",
    "# svc_pred=cross_val_predict(svc, X_train, y_train, cv=5, method=\"decision_function\")\n",
    "# tree_pred=cross_val_predict(tree_clf, X_train, y_train, cv=5)\n",
    "\n",
    "# print(\"Logistic Regression: \", roc_auc_score(y_train, log_reg_pred))\n",
    "# print(\"KNears Neighbors: \", roc_auc_score(y_train, knears_pred))\n",
    "# print(\"Support Vector Classifier: \", roc_auc_score(y_train, svc_pred))\n",
    "# print(\"Decision Tree Classifier: \", roc_auc_score(y_train, tree_pred))\n",
    "\n",
    "# log_fpr, log_tpr, log_thresold=roc_curve(y_train, log_reg_pred)\n",
    "# knear_fpr, knear_tpr, knear_threshold=roc_curve(y_train, knears_pred)\n",
    "# svc_fpr, svc_tpr, svc_threshold=roc_curve(y_train, svc_pred)\n",
    "# tree_fpr, tree_tpr, tree_threshold=roc_curve(y_train, tree_pred)\n",
    "\n",
    "\n",
    "# def graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n",
    "#     plt.figure(figsize=(16, 8))\n",
    "#     plt.title(\"ROC Curve \\n Top 4 Classifiers\", fontsize=12)\n",
    "#     plt.plot(log_fpr, log_tpr, label=f\"Logistic Regression Classifier Score: {roc_auc_score(y_train, log_reg_pred):.4f}\")\n",
    "#     plt.plot(knear_fpr, knear_tpr, label=f\"KNears Neighbors Classifier Score: {roc_auc_score(y_train, knears_pred):.4f}\")\n",
    "#     plt.plot(svc_fpr, svc_tpr, label=f\"Support Vector Classifier Score: {roc_auc_score(y_train, svc_pred):.4f}\")\n",
    "#     plt.plot(tree_fpr, tree_tpr, label=f\"Decision Tree Classifier Score: {roc_auc_score(y_train, tree_pred):.4f}\")\n",
    "#     plt.plot([0, 1], [0, 1], \"k--\")\n",
    "#     plt.axis([-0.01, 1, 0, 1])\n",
    "#     plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "#     plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "#     plt.annotate(\"Minimum ROC Score of 50% \\n (This is the minimum score to get)\", xy=(0.5, 0.5), xytext=(0.6, 0.3), arrowprops=dict(facecolor=\"#6E726D\", shrink=0.05))\n",
    "#     plt.legend()\n",
    "\n",
    "# graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\n",
    "# plt.show()\n",
    "\n",
    "# def logistic_roc_curve(log_fpr, log_tpr):\n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     plt.title(\"Logistic Regression ROC Curve\", fontsize=12)\n",
    "#     plt.plot(log_fpr, log_tpr, \"b-\", linewidth=2)\n",
    "#     plt.plot([0, 1], [0, 1], \"r--\")\n",
    "#     plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "#     plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "#     plt.axis([-0.01, 1, 0, 1])\n",
    "\n",
    "# logistic_roc_curve(log_fpr, log_tpr)\n",
    "# plt.show()\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def compute_cross_val_predictions(classifiers: Dict[str, object], X, y, cv: int = 5, use_decision_fn: List[str] = None) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Compute cross-validated predictions or decision function scores for each classifier.\n",
    "\n",
    "    Args:\n",
    "        classifiers: Mapping of classifier names to instantiated models.\n",
    "        X: Feature matrix.\n",
    "        y: Target vector.\n",
    "        cv: Number of folds for cross-validation.\n",
    "        use_decision_fn: List of classifier names to use decision_function instead of predict.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping classifier names to prediction score arrays.\n",
    "    \"\"\"\n",
    "    preds = {}\n",
    "    for name, clf in classifiers.items():\n",
    "        # Choose method: decision_function if available/requested, else default predict\n",
    "        if use_decision_fn and name in use_decision_fn and hasattr(clf, 'decision_function'):\n",
    "            preds[name] = cross_val_predict(clf, X, y, cv=cv, method='decision_function')\n",
    "        else:\n",
    "            preds[name] = cross_val_predict(clf, X, y, cv=cv)\n",
    "    return preds\n",
    "\n",
    "\n",
    "def compute_scores_and_curves(preds: Dict[str, np.ndarray], y) -> Tuple[pd.DataFrame, Dict[str, Tuple[np.ndarray, np.ndarray, np.ndarray]]]:\n",
    "    \"\"\"\n",
    "    Calculate ROC AUC scores and ROC curve points for each classifier.\n",
    "\n",
    "    Args:\n",
    "        preds: Mapping of classifier names to predicted scores or labels.\n",
    "        y: True target vector.\n",
    "\n",
    "    Returns:\n",
    "        scores_df: DataFrame with columns ['classifier', 'roc_auc'].\n",
    "        roc_data: Mapping of classifier names to (fpr, tpr, thresholds).\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    roc_data = {}\n",
    "    for name, scores in preds.items():\n",
    "        # Compute AUC\n",
    "        records.append({'classifier': name, 'roc_auc': roc_auc_score(y, scores)})\n",
    "\n",
    "        # Compute ROC curve\n",
    "        fpr, tpr, thresh = roc_curve(y, scores)\n",
    "        roc_data[name] = (fpr, tpr, thresh)\n",
    "\n",
    "    scores_df = pd.DataFrame.from_records(records)\n",
    "    return scores_df, roc_data\n",
    "\n",
    "\n",
    "def plot_multiple_roc(roc_data: Dict[str, Tuple[np.ndarray, np.ndarray, np.ndarray]], scores_df: pd.DataFrame, figsize: Tuple[int, int] = (16, 8), title: str = 'ROC Curve - Multiple Classifiers') -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot ROC curves for multiple classifiers on one figure.\n",
    "\n",
    "    Args:\n",
    "        roc_data: Mapping of classifier names to ROC curve arrays.\n",
    "        scores_df: DataFrame with 'classifier' and 'roc_auc' columns.\n",
    "        figsize: Figure size tuple.\n",
    "        title: Plot title.\n",
    "\n",
    "    Returns:\n",
    "        Matplotlib Figure object.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title, fontsize=14)\n",
    "\n",
    "    # Plot each ROC curve with AUC in label\n",
    "    for _, row in scores_df.iterrows():\n",
    "        name = row['classifier']\n",
    "        auc = row['roc_auc']\n",
    "        fpr, tpr, _ = roc_data[name]\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc:.4f})\")\n",
    "\n",
    "    # Diagonal chance line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "    plt.xlim([-0.01, 1])\n",
    "    plt.ylim([0, 1.01])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc='lower right')\n",
    "    return plt.gcf()\n",
    "\n",
    "\n",
    "def plot_single_roc(roc_data: Tuple[np.ndarray, np.ndarray, np.ndarray], title: str = 'ROC Curve', figsize: Tuple[int, int] = (12, 8)) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Plot a single ROC curve.\n",
    "\n",
    "    Args:\n",
    "        roc_data: Tuple of (fpr, tpr, thresholds).\n",
    "        title: Plot title.\n",
    "        figsize: Figure size tuple.\n",
    "\n",
    "    Returns:\n",
    "        Matplotlib Figure object.\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_data\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.plot(fpr, tpr, linestyle='-', lw=2)\n",
    "    plt.plot([0, 1], [0, 1], 'r--', lw=1)\n",
    "    plt.xlim([-0.01, 1])\n",
    "    plt.ylim([0, 1.01])\n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.grid(True)\n",
    "    return plt.gcf()\n",
    "\n",
    "\n",
    "classifiers = {'Logistic Regression': log_reg, 'KNeighbors': knears_neighbors, 'SVC': svc, 'Decision Tree': tree_clf}\n",
    "preds = compute_cross_val_predictions(classifiers, X_train, y_train, cv=5, use_decision_fn=['Logistic Regression', 'SVC'])\n",
    "scores_df, roc_data = compute_scores_and_curves(preds, y_train)\n",
    "print(scores_df)\n",
    "fig_multi = plot_multiple_roc(roc_data, scores_df)\n",
    "fig_single = plot_single_roc(roc_data['Logistic Regression'], 'Logistic Regression ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f4d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, threshold=precision_recall_curve(y_train, preds[\"log_reg_pred\"])\n",
    "y_pred=log_reg.predict(X_train)\n",
    "\n",
    "# Overfitting Case\n",
    "print(\"---\" * 45)\n",
    "print(\"Overfitting: \\n\")\n",
    "print(f\"Recall Score: {recall_score(y_train, y_pred):.2f}\")\n",
    "print(f\"Precision Score: {precision_score(y_train, y_pred):.2f}\")\n",
    "print(f\"F1 Score: {f1_score(y_train, y_pred):.2f}\")\n",
    "print(f\"Accuracy Score: {accuracy_score(y_train, y_pred):.2f}\")\n",
    "print(\"---\" * 45)\n",
    "\n",
    "# How it should look like\n",
    "print(\"---\" * 45)\n",
    "print(\"How it should be: \\n\")\n",
    "print(f\"Accuracy Score: {np.mean(undersample_accuracy):.2f}\")\n",
    "print(f\"Precision Score: {np.mean(undersample_precision):.2f}\")\n",
    "print(f\"Recall Score: {np.mean(undersample_recall):.2f}\")\n",
    "print(f\"F1 Score: {np.mean(undersample_f1):.2f}\")\n",
    "print(\"---\" * 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample_y_score=log_reg.decision_function(original_Xtest)\n",
    "undersample_average_precision=average_precision_score(original_ytest, undersample_y_score)\n",
    "print(f\"Average precision-recall score: {undersample_average_precision:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c924a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12, 6))\n",
    "precision, recall, _=precision_recall_curve(original_ytest, undersample_y_score)\n",
    "\n",
    "plt.step(recall, precision, color=\"#004a93\", alpha=0.2, where=\"post\")\n",
    "plt.fill_between(recall, precision, step=\"post\", alpha=0.2, color=\"#48a6ff\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(f\"UnderSampling Precision-Recall curve:\\nAverage Precision-Recall Score={undersample_average_precision:0.2f}\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5caefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Length of X_train: {len(original_Xtrain)} | Length of y_train: {len(original_ytrain)}\")\n",
    "print(f\"Length of X_test: {len(original_Xtest)} | Length of y_test: {len(original_ytest)}\")\n",
    "\n",
    "# List to append the score and then find the average\n",
    "accuracy_lst=[]\n",
    "precision_lst=[]\n",
    "recall_lst=[]\n",
    "f1_lst=[]\n",
    "auc_lst=[]\n",
    "\n",
    "# Classifier with optimal parameters\n",
    "# log_reg_sm=grid_log_reg.best_estimator_\n",
    "log_reg_sm=LogisticRegression()\n",
    "rand_log_reg=RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n",
    "\n",
    "# Implementing SMOTE Technique\n",
    "# Cross Validating the right way\n",
    "# Parameters\n",
    "log_reg_params={\"penalty\": [\"l1\", \"l2\"], \"C\": [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "for train, test in sss.split(original_Xtrain, original_ytrain):\n",
    "    pipeline=imbalanced_make_pipeline(SMOTE(sampling_strategy=\"minority\"), rand_log_reg)  # SMOTE happens during Cross Validation not before..\n",
    "    model=pipeline.fit(original_Xtrain[train], original_ytrain[train])\n",
    "    best_est=rand_log_reg.best_estimator_\n",
    "    prediction=best_est.predict(original_Xtrain[test])\n",
    "\n",
    "    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n",
    "    precision_lst.append(precision_score(original_ytrain[test], prediction))\n",
    "    recall_lst.append(recall_score(original_ytrain[test], prediction))\n",
    "    f1_lst.append(f1_score(original_ytrain[test], prediction))\n",
    "    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n",
    "\n",
    "print(\"---\" * 45)\n",
    "print(f\"\\naccuracy: {np.mean(accuracy_lst)} \\nprecision: {np.mean(precision_lst)} \\nrecall: {np.mean(recall_lst)} \\nf1: {np.mean(f1_lst)}\")\n",
    "print(\"---\" * 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db57569",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=[\"No Fraud\", \"Fraud\"]\n",
    "smote_prediction=best_est.predict(original_Xtest)\n",
    "print(classification_report(original_ytest, smote_prediction, target_names=labels))\n",
    "\n",
    "y_score=best_est.decision_function(original_Xtest)\n",
    "average_precision=average_precision_score(original_ytest, y_score)\n",
    "print(f\"Average precision-recall score: {average_precision:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ebc06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(12, 6))\n",
    "precision, recall, _=precision_recall_curve(original_ytest, y_score)\n",
    "\n",
    "plt.step(recall, precision, color=\"r\", alpha=0.2, where=\"post\")\n",
    "plt.fill_between(recall, precision, step=\"post\", alpha=0.2, color=\"#F59B00\")\n",
    "\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title(f\"OverSampling Precision-Recall curve: \\nAverage Precision-Recall Score ={average_precision:0.2f}\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8247f37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE Technique (OverSampling) After splitting and Cross Validating\n",
    "sm=SMOTE(sampling_strategy=\"minority\", random_state=42)\n",
    "\n",
    "# This will be the data were we are going to\n",
    "Xsm_train, ysm_train=sm.fit_resample(original_Xtrain, original_ytrain)\n",
    "\n",
    "# We Improve the score by 2% points approximately\n",
    "# Implement GridSearchCV and the other models.\n",
    "\n",
    "# Logistic Regression\n",
    "t0=time.time()\n",
    "log_reg_sm=grid_log_reg.best_estimator_\n",
    "log_reg_sm.fit(Xsm_train, ysm_train)\n",
    "t1=time.time()\n",
    "print(f\"Fitting oversample data took :{t1 - t0} sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3675133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression fitted using SMOTE technique\n",
    "y_pred_log_reg=log_reg_sm.predict(X_test)\n",
    "\n",
    "# Other models fitted with UnderSampling\n",
    "y_pred_knear=knears_neighbors.predict(X_test)\n",
    "y_pred_svc=svc.predict(X_test)\n",
    "y_pred_tree=tree_clf.predict(X_test)\n",
    "\n",
    "log_reg_cf=confusion_matrix(y_test, y_pred_log_reg)\n",
    "kneighbors_cf=confusion_matrix(y_test, y_pred_knear)\n",
    "svc_cf=confusion_matrix(y_test, y_pred_svc)\n",
    "tree_cf=confusion_matrix(y_test, y_pred_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c599875f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax=plt.subplots(2, 2, figsize=(22, 12))\n",
    "\n",
    "sns.heatmap(log_reg_cf, ax=ax[0][0], annot=True, cmap=plt.cm.copper)\n",
    "ax[0, 0].set_title(\"Logistic Regression \\n Confusion Matrix\", fontsize=12)\n",
    "ax[0, 0].set_xticklabels([\"\", \"\"], fontsize=12, rotation=90)\n",
    "ax[0, 0].set_yticklabels([\"\", \"\"], fontsize=12, rotation=360)\n",
    "\n",
    "sns.heatmap(kneighbors_cf, ax=ax[0][1], annot=True, cmap=plt.cm.copper)\n",
    "ax[0][1].set_title(\"KNearsNeighbors \\n Confusion Matrix\", fontsize=12)\n",
    "ax[0][1].set_xticklabels([\"\", \"\"], fontsize=12, rotation=90)\n",
    "ax[0][1].set_yticklabels([\"\", \"\"], fontsize=12, rotation=360)\n",
    "\n",
    "sns.heatmap(svc_cf, ax=ax[1][0], annot=True, cmap=plt.cm.copper)\n",
    "ax[1][0].set_title(\"Suppor Vector Classifier \\n Confusion Matrix\", fontsize=12)\n",
    "ax[1][0].set_xticklabels([\"\", \"\"], fontsize=12, rotation=90)\n",
    "ax[1][0].set_yticklabels([\"\", \"\"], fontsize=12, rotation=360)\n",
    "\n",
    "sns.heatmap(tree_cf, ax=ax[1][1], annot=True, cmap=plt.cm.copper)\n",
    "ax[1][1].set_title(\"DecisionTree Classifier \\n Confusion Matrix\", fontsize=12)\n",
    "ax[1][1].set_xticklabels([\"\", \"\"], fontsize=12, rotation=90)\n",
    "ax[1][1].set_yticklabels([\"\", \"\"], fontsize=12, rotation=360)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic Regression:\", classification_report(y_test, y_pred_log_reg))\n",
    "print(\"KNears Neighbors:\", classification_report(y_test, y_pred_knear))\n",
    "print(\"Support Vector Classifier:\", classification_report(y_test, y_pred_svc))\n",
    "print(\"Support Vector Classifier:\", classification_report(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466c6d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Score in the test set of logistic regression\n",
    "# Logistic Regression with Under-Sampling\n",
    "y_pred=log_reg.predict(X_test)\n",
    "undersample_score=accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Logistic Regression with SMOTE Technique (Better accuracy with SMOTE t)\n",
    "y_pred_sm=best_est.predict(original_Xtest)\n",
    "oversample_score=accuracy_score(original_ytest, y_pred_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d1f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d={\"Technique\": [\"Random UnderSampling\", \"Oversampling (SMOTE)\"], \"Score\": [undersample_score, oversample_score]}\n",
    "final_df=pd.DataFrame(data=d)\n",
    "\n",
    "# Move column\n",
    "score=final_df[\"Score\"]\n",
    "final_df.drop(\"Score\", axis=1, inplace=True)\n",
    "final_df.insert(1, \"Score\", score)\n",
    "\n",
    "# Note how high is accuracy score it can be misleading!\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a69bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs=X_train.shape[1]\n",
    "undersample_model=Sequential([Dense(n_inputs, input_shape=(n_inputs,), activation=\"relu\"), Dense(32, activation=\"relu\"), Dense(2, activation=\"softmax\")])\n",
    "undersample_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d845bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample_model.compile(Adam(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "undersample_model.fit(X_train,y_train,validation_split=0.2,batch_size=25,epochs=20,shuffle=True,verbose=2)\n",
    "undersample_predictions=undersample_model.predict(original_Xtest, batch_size=200, verbose=0)\n",
    "undersample_fraud_predictions=np.argmax(undersample_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5019dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a confusion matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title=\"Confusion matrix\", cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm=cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print(\"Confusion matrix, without normalization\")\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.colorbar()\n",
    "    tick_marks=np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt=\".2f\" if normalize else \"d\"\n",
    "    thresh=cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "\n",
    "\n",
    "undersample_cm=confusion_matrix(original_ytest, undersample_fraud_predictions)\n",
    "actual_cm=confusion_matrix(original_ytest, original_ytest)\n",
    "labels=[\"No Fraud\", \"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3519640",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(16, 8))\n",
    "\n",
    "fig.add_subplot(221)\n",
    "plot_confusion_matrix(undersample_cm, labels, title=\"Random UnderSample \\n Confusion Matrix\", cmap=plt.cm.Reds)\n",
    "\n",
    "fig.add_subplot(222)\n",
    "plot_confusion_matrix(actual_cm, labels, title=\"Confusion Matrix \\n (with 100% accuracy)\", cmap=plt.cm.Greens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c147dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs=Xsm_train.shape[1]\n",
    "\n",
    "oversample_model=Sequential([Dense(n_inputs, input_shape=(n_inputs,), activation=\"relu\"), Dense(32, activation=\"relu\"), Dense(2, activation=\"softmax\")])\n",
    "oversample_model.compile( Adam(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "oversample_model.fit( Xsm_train, ysm_train, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=2)\n",
    "oversample_predictions=oversample_model.predict( original_Xtest, batch_size=200, verbose=0)\n",
    "oversample_fraud_predictions=np.argmax(oversample_predictions, axis=1)\n",
    "\n",
    "oversample_smote=confusion_matrix(original_ytest, oversample_fraud_predictions)\n",
    "actual_cm=confusion_matrix(original_ytest, original_ytest)\n",
    "labels=[\"No Fraud\", \"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a50869",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(16, 8))\n",
    "\n",
    "fig.add_subplot(221)\n",
    "plot_confusion_matrix(oversample_smote, labels, title=\"OverSample (SMOTE) \\n Confusion Matrix\", cmap=plt.cm.Oranges)\n",
    "\n",
    "fig.add_subplot(222)\n",
    "plot_confusion_matrix(actual_cm, labels, title=\"Confusion Matrix \\n (with 100% accuracy)\", cmap=plt.cm.Greens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
